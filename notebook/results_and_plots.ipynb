{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A hidden Markov model for lymphatic tumour progression in head and neck cancer\n",
    "\n",
    "Roman LudwigÂ¹*, Bertrand PouymayouÂ¹, Panagiotis BalermpasÂ¹ and Jan UnkelbachÂ¹\n",
    "\n",
    "Â¹ Departement of Radiation Oncology, University Hospital Zurich, Switzerland \\\n",
    "\\* [roman.ludwig@usz.ch](mailto:roman.ludwig@usz.ch)\n",
    "\n",
    "***\n",
    "\n",
    "## Abstract [ðŸ”—](https://www.nature.com/articles/s41598-021-91544-1)\n",
    "\n",
    "Currently , elective clinical target volume (CTV-N) definition for head & neck squamous cell carcinoma (HNSCC) is mostly based on the prevalence of nodal involvement for a given tumor location. In this work, we propose a probabilistic model for lymphatic metastatic spread that can quantify the risk of microscopic involvement in lymph node levels (LNL) given the location of macroscopic metastases and T-stage. This may allow for further personalized CTV-N definition based on an individual patientâ€™s state of disease. \\\n",
    "We model the patient's state of metastatic lymphatic progression as a collection of hidden binary random variables that indicate the involvement of LNLs. In addition, each LNL is associated with observed binary random variables that indicate whether macroscopic metastases are detected. A hidden Markov model (HMM) is used to compute the probabilities of transitions between states over time. The underlying graph of the HMM represents the anatomy of the lymphatic drainage system. Learning of the transition probabilities is done via Markov chain Monte Carlo sampling and is based on a dataset of HNSCC patients in whom involvement of individual LNLs was report-ed. \\\n",
    "The model is demonstrated for ipsilateral metastatic spread in oropharyngeal HNSCC patients. We demonstrate the model's capability to quantify the risk of microscopic involvement in levels III and IV, depending on whether macroscopic metastases are observed in the upstream levels II and III, and depending on T-stage. \\\n",
    "In conclusion, the statistical model of lymphatic progression may inform future, more personal-ized, guidelines on which LNL to include in the elective CTV. However, larger multi-institutional datasets for model parameter learning are required for that. \n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains all the code we ran to produce results and plots for our paper. It is intended to be read alongside the paper if one wants to understand better how exactly we implemented and used the methodology introduced in the paper. However, this notebook is NOT a stand-alone work but only a supplement.\n",
    "\n",
    "### Imports\n",
    "\n",
    "First, we import some libraries that are necessary for our implementation. [`lymph`](https://lymph-model.readthedocs.io/en/latest/) is the package we wrote, while [`corner`](https://corner.readthedocs.io/en/latest/) and [`emcee`](https://emcee.readthedocs.io/en/stable/) (see also the corresponding [arXiv paper](https://arxiv.org/abs/1202.3665) on this package) are both packages by Dan Foreman-Mackey & contributors. [`matplotlib`](https://matplotlib.org/stable/index.html) is an extensive and powerful plotting library. All other packages are standard and included with any default python installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional requirements to install for running this notebook\n",
    "!pip install -r notebook_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stuff\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import datetime as dt\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gs\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cycler import cycler\n",
    "import corner\n",
    "\n",
    "# sampling\n",
    "import emcee\n",
    "\n",
    "# our package\n",
    "import lymph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The variables below are meant to be constants. `MAX_T` is the length of the used binomial time-prior. `DRAW_SAMPLES` is a `bool` that defines whether new samples will be drawn for the plots and computations or alreadz drawn samples should be loaded from file. `SEED` is the seed for the random number generator in `numpy` and aims at reproducability. `SAVE_FIGURES` defines whether or not generated figures should be saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_T = 10\n",
    "DRAW_SAMPLES = True\n",
    "SEED = 42\n",
    "SAVE_FIGURES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors\n",
    "\n",
    "We chose the colors of the University Hospital Zurich's corporate design for the default colors of our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USZ colors\n",
    "usz_blue = '#005ea8'\n",
    "usz_green = '#00afa5'\n",
    "usz_red = '#ae0060'\n",
    "usz_orange = '#f17900'\n",
    "usz_gray = '#c5d5db'\n",
    "\n",
    "# colormaps\n",
    "white_to_blue  = LinearSegmentedColormap.from_list(\"white_to_blue\", \n",
    "                                                   [\"#ffffff\", usz_blue], \n",
    "                                                   N=256)\n",
    "white_to_green = LinearSegmentedColormap.from_list(\"white_to_green\", \n",
    "                                                   [\"#ffffff\", usz_green], \n",
    "                                                   N=256)\n",
    "green_to_red   = LinearSegmentedColormap.from_list(\"green_to_red\", \n",
    "                                                   [usz_green, usz_red], \n",
    "                                                   N=256)\n",
    "\n",
    "h = usz_gray.lstrip('#')\n",
    "gray_rgba = tuple(int(h[i:i+2], 16) / 255. for i in (0, 2, 4)) + (1.0,)\n",
    "tmp = LinearSegmentedColormap.from_list(\"tmp\", [usz_green, usz_red], N=128)\n",
    "tmp = tmp(np.linspace(0., 1., 128))\n",
    "tmp = np.vstack([np.array([gray_rgba]*128), tmp])\n",
    "halfGray_halfGreenToRed = ListedColormap(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Settings\n",
    "\n",
    "Here we define a function to consistently set the size of our plots and we load the default settings regarding font size etc from an `.mplstyle` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"./lymph.mplstyle\")\n",
    "\n",
    "def set_size(width=\"single\", unit=\"cm\", ratio=\"golden\"):\n",
    "    if width == \"single\":\n",
    "        width = 10\n",
    "    elif width == \"full\":\n",
    "        width = 16\n",
    "    else:\n",
    "        try:\n",
    "            width = width\n",
    "        except:\n",
    "            width = 10\n",
    "            \n",
    "    if unit == \"cm\":\n",
    "        width = width / 2.54\n",
    "        \n",
    "    if ratio == \"golden\":\n",
    "        ratio = 1.618\n",
    "    else:\n",
    "        ratio = ratio\n",
    "    \n",
    "    try:\n",
    "        height = width / ratio\n",
    "    except:\n",
    "        height = width / 1.618\n",
    "        \n",
    "    return (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only works on Windows when using WSL 2\n",
    "font_dirs = [\"/usr/share/fonts/truetype/dejavu\", \"/mnt/c/Windows/Fonts\"]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we use here was reconstructed by [Pouymayou et al.](#pouymayou) from [Sanguineti et al.](#sanguineti). Here, it is loaded from file. Note that $N_0$ patients were added to make up 30% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and risk estimation from Pouymayou et al [2]\n",
    "pouymayou_params = [0.061, 0.638, 0.094, 0.057, 0.08, 0.331, 0.242]\n",
    "pouymayou_MLrisk = np.array([[ 1.53,  1.64,  1.66,  1.56], \n",
    "                             [24.67, 81.55, 89.64, 39.06], \n",
    "                             [ 4.48,  9.97, 59.93, 38.75], \n",
    "                             [ 1.83,  2.25,  6.05,  4.44]]) / 100.\n",
    "\n",
    "# data reconstructed from Sanguineti et al [1] (without N0 patients)\n",
    "data = pd.read_csv(\"./data/2009_sanguineti.csv\", \n",
    "                   header=None, \n",
    "                   names=['I', 'II', 'III', 'IV'])\n",
    "\n",
    "# inserting info about the \"T-stage\"\n",
    "data.insert(0, \"t_stage\", [\"early\"] * data.shape[0])\n",
    "\n",
    "columns = pd.MultiIndex.from_arrays([['info', 'path', 'path', 'path', 'path'], \n",
    "                                     ['t_stage', 'I', 'II', 'III', 'IV']])\n",
    "data = pd.DataFrame(data.values.tolist(), columns=columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Table 1:__ Rows of patients and columns of T-stage, as well as nodal involvement patterns. Reconstructed from [[1]](#sanguineti).*\n",
    "\n",
    "## Inference\n",
    "\n",
    "In this section we will set up everything necessary to perform inference on the dataset recostructed by [[2]](#pouymayou) from [[1]](#sanguineti).\n",
    "\n",
    "### Lymphatic Network\n",
    "\n",
    "Here, we need to define the underlying anatomical network of lymph node levels (LNLs), as it is also defined in [[2]](#pouymayou). \n",
    "\n",
    "The tumor and every LNL are represented by a key in a dictionary called `graph` each. The respective value in the dictionary is a list of nodes it drains to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {('tumor', 'primary')  : ['I', 'II', 'III', 'IV'], \n",
    "         ('lnl', 'I')          : ['II'], \n",
    "         ('lnl', 'II')         : ['III'], \n",
    "         ('lnl', 'III')        : ['IV'], \n",
    "         ('lnl', 'IV')         : []}\n",
    "\n",
    "model = lymph.System(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Prior\n",
    "We need to choose a time prior for the parameter learning. A [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) was chosen for its intuitively meaningful shape and simple structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=set_size());\n",
    "\n",
    "for i,p in enumerate([0.4, 0.55, 0.7]):\n",
    "    ax.plot(np.arange(MAX_T+1), \n",
    "            sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, p), \n",
    "            \"o-\", label=f\"$p = {{{p}}}$\")\n",
    "    \n",
    "ax.set_xlim([0,10]);\n",
    "ax.set_ylim([-0.02,0.35])\n",
    "ax.set_xlabel(\"time step $t$\");\n",
    "ax.set_ylabel(r\"$p(t)$\");\n",
    "ax.tick_params();\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 1:__ Binomial distribution with three different p-parameters. They represent the probability of diagnosis at time-step $t$ for three different scenarios, like T-category.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of time priors for the sampling process\n",
    "p_early = 0.4\n",
    "time_dists = {}\n",
    "time_dists[\"early\"] = sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, p_early)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning (HMM)\n",
    "\n",
    "Using the anatomical model along with the data, we can now load the data into the lymph system class. To do so we must first assign a dictionary called `modality_spsn` to the lymphatic system or pass it together with the data in the `load_data` method. **``spsn``** stands for **sp**ecificity and **s**e**n**sitivity. In that dictionary, one has to define a list ``[specificty, sensitivity]`` for each diagnostic modality one is interested in. So if we, for example, have MRI and CT data, then we would pass a dictionary like this:\n",
    "\n",
    "```python\n",
    "modality_spsn = {\n",
    "    \"MRI\": [spec_MRI, sens_MRI], \n",
    "    \"CT\" : [spec_CT , sens_CT ]\n",
    "}\n",
    "```\n",
    "\n",
    "Note however, that the keys of this dictionary must be diagnostic modalities that are also present in the dataset. More precisely, they must be the overarching categories in the ``MultiIndex`` under which one then finds the individual LNLs.\n",
    "\n",
    "Finally, we can use the likelihood function that is built into the `lymph` package together with the sampling implementation `emcee` to infer the base probabilities $b_{v}$ and transition probabilities $t_{\\operatorname{pa}(v) \\rightarrow v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define specificity and sensitivity for diagnostic modalities\n",
    "modality_spsn = {\"path\": [1., 1.]}\n",
    "model.modalities = modality_spsn\n",
    "\n",
    "# load data\n",
    "model.patient_data = data\n",
    "\n",
    "# check if likelihood works\n",
    "spread_probs = np.random.uniform(size=(7,))\n",
    "llh = model.log_likelihood(\n",
    "    spread_probs, t_stages=[\"early\"], \n",
    "    time_dists=time_dists, \n",
    "    mode=\"HMM\"\n",
    ")\n",
    "print(llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the sampler\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "if DRAW_SAMPLES:\n",
    "    # starting point\n",
    "    np.random.seed(SEED)\n",
    "    initial_spread_probs = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "\n",
    "    # the actual sampling round\n",
    "    if __name__ == \"__main__\":\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(\n",
    "                nwalker, ndim, \n",
    "                model.log_likelihood, \n",
    "                kwargs={\"t_stages\": [\"early\"], \"time_dists\": time_dists}, \n",
    "                moves=moves, pool=pool\n",
    "            )\n",
    "            sampler.run_mcmc(initial_spread_probs, nstep, progress=True)\n",
    "\n",
    "    # extracting 200,000 of the 400,000 samples\n",
    "    samples_HMM = sampler.get_chain(flat=True, discard=burnin)\n",
    "\n",
    "    # saving the sampled data to disk for later convenience\n",
    "    np.save(\"./samples/HMM.npy\", samples_HMM)\n",
    "    \n",
    "else:\n",
    "    # loading in case we don't want to draw all the samples again\n",
    "    samples_HMM = np.load(\"./samples/HMM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRAW_SAMPLES:\n",
    "    # check acceptance faction of the sampler to get an indication on whether sth\n",
    "    # went wrong or not\n",
    "    ar = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"the HMM sampler accepted {ar * 100 :.2f} % of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r\"$\\tilde{b}_1$\", r\"$\\tilde{b}_2$\", \n",
    "          r\"$\\tilde{b}_3$\", r\"$\\tilde{b}_4$\", \n",
    "          r\"$\\tilde{t}_{12}$\", r\"$\\tilde{t}_{23}$\", r\"$\\tilde{t}_{34}$\"]\n",
    "\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=1))\n",
    "\n",
    "# using the corner plot package\n",
    "corner.corner(samples_HMM, labels=labels, smooth=True, fig=fig, \n",
    "              hist_kwargs={'histtype': 'stepfilled', 'color': usz_blue}, \n",
    "              **{'plot_datapoints': False, 'no_fill_contours': True, \n",
    "                 \"density_cmap\": white_to_blue.reversed(), \n",
    "                 \"contour_kwargs\": {\"colors\": \"k\"}, \n",
    "                 \"levels\": np.array([0.2, 0.5, 0.8])}, \n",
    "              show_titles=True, title_kwargs={\"fontsize\": \"medium\"});\n",
    "\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.grid(False)\n",
    "    \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/corner_HMM.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/corner_HMM.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 2:__ Corner plot of the sampled parameters for the HMM model parameters. The histograms on the diagonal show the 1D marginals, while the lower triangle shows all possible combinations of 2D marginals. The black lines are the isolines enclosing 20%, 50% and 80% of the sampled points respectively. Correlations between the parameters can at most be seen between $\\tilde{t}_{23}$ and $\\tilde{b}_3$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Matrix\n",
    "\n",
    "We can now set the hidden Markov model's parameters to the expected value of the inferred parameters and look at the resulting transition matrix $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "model.spread_probs = np.mean(samples_HMM, axis=0)\n",
    "\n",
    "# modify the transition matrix for nicer coloring\n",
    "mod_A = -1 * np.ones_like(model.transition_matrix)\n",
    "for key, nums in model.allowed_transitions.items():\n",
    "    for i in nums:\n",
    "        mod_A[key, i] = model.transition_matrix[key, i]\n",
    "\n",
    "# plot the transition matrix\n",
    "fig, ax = plt.subplots(figsize=set_size(ratio=1.), \n",
    "                       constrained_layout=True);\n",
    "\n",
    "h = ax.imshow(mod_A, cmap=halfGray_halfGreenToRed, vmin=-1., vmax=1.);\n",
    "ax.set_xticks(range(len(model.state_list)));\n",
    "ax.set_xticklabels(model.state_list, rotation=-90, fontsize=\"small\");\n",
    "ax.set_yticks(range(len(model.state_list)));\n",
    "ax.set_yticklabels(model.state_list, fontsize=\"small\");\n",
    "ax.tick_params(direction=\"out\")\n",
    "ax.grid(False)\n",
    "\n",
    "# label the non-zero entries with their probability in %\n",
    "for i in range(len(model.state_list)):\n",
    "    for j in range(len(model.state_list)):\n",
    "        if mod_A[i,j] > 0.:\n",
    "            ax.text(j,i, f\"{mod_A[i,j]*100:.1f}\", ha=\"center\", va=\"center\", \n",
    "                    color=\"white\", fontsize=\"x-small\")\n",
    "            \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/transition_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/transition_matrix.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 3:__ Transition matrix $\\mathbf{A}$. All gray pixels in this image correspond to entries in the matrix being zero. The colored pixels take on values $\\in [0,1]$ which are here overlayed in %. The exact values stem from the mean of the learned parameters displayed above. The exact shape of the grey â€œmaskâ€ depends on how one orders the states*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution Plots\n",
    "\n",
    "We can also take a look at how this system evolves over the defined time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array containing the risk for each state...\n",
    "state_array = np.zeros(shape=(MAX_T+1, len(model.state_list)), dtype=float)\n",
    "# ...weighted with the probability for that time\n",
    "state_array_weighted = np.zeros_like(state_array, dtype=float)\n",
    "state_array_summed = np.zeros_like(state_array, dtype=float)\n",
    "# starting state\n",
    "start = np.zeros(shape=(len(model.state_list),))\n",
    "start[0] = 1.\n",
    "\n",
    "# manually evolving the system and storing all intermediate states\n",
    "time_dist = sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, 0.4)\n",
    "for t,p in enumerate(time_dist):\n",
    "    state_array[t] = start\n",
    "    state_array_weighted[t] = p * start\n",
    "    state_array_summed[t] = np.ones(shape=(1,t+1)) @ state_array_weighted[:t+1]\n",
    "    state_array_summed[t] = state_array_summed[t] / np.sum(state_array_summed[t])\n",
    "    start = start @ model.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these arrays define which states we need to marginalize over when we are \n",
    "# interested in a particual LNL's risk of involvement\n",
    "lnl_I_arr = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], \n",
    "                     dtype=float)\n",
    "lnl_II_arr = np.array([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1], \n",
    "                      dtype=float)\n",
    "lnl_III_arr = np.array([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1], \n",
    "                       dtype=float)\n",
    "lnl_IV_arr = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], \n",
    "                      dtype=float)\n",
    "\n",
    "lnl_I = state_array @ lnl_I_arr\n",
    "lnl_II = state_array @ lnl_II_arr \n",
    "lnl_III = state_array @ lnl_III_arr \n",
    "lnl_IV = state_array @ lnl_IV_arr\n",
    "\n",
    "lnl_concat = np.vstack([lnl_I, lnl_II, lnl_III, lnl_IV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marg_II = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "marg_III = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "marg_II_and_III = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "marg_IV = np.zeros(shape=(len(samples_HMM[::50])))\n",
    "\n",
    "np.random.seed(SEED)\n",
    "for i, spread_probs in enumerate(np.random.permutation(samples_HMM[::50])):\n",
    "    model.spread_probs = spread_probs\n",
    "    marg_II[i] = 100 * model.risk(\n",
    "        inv=np.array([None, 1, None, None]),\n",
    "        diagnoses={\"path\": np.array([None, None, None, None])},\n",
    "        time_dist=time_dist\n",
    "    )\n",
    "    marg_III[i] = 100 * model.risk(\n",
    "        inv=np.array([None, None, 1, None]),\n",
    "        diagnoses={\"path\": np.array([None, None, None, None])},\n",
    "        time_dist=time_dist\n",
    "    )\n",
    "    marg_II_and_III[i] = 100 * model.risk(\n",
    "        inv=np.array([None, 1, 1, None]),\n",
    "        diagnoses={\"path\": np.array([None, None, None, None])},\n",
    "        time_dist=time_dist\n",
    "    )\n",
    "    marg_IV[i] = 100 * model.risk(\n",
    "        inv=np.array([None, None, None, 1]),\n",
    "        diagnoses={\"path\": np.array([None, None, None, None])},\n",
    "        time_dist=time_dist\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginalization for involvements of \"at least LNL x\" or \"only LNL x\" \n",
    "only_II = state_array[:,4]\n",
    "atleast_II = (state_array[:,4] + state_array[:,5] + state_array[:,6] \n",
    "              + state_array[:,7] + state_array[:,12] + state_array[:,13] \n",
    "              + state_array[:,14] + state_array[:,15])\n",
    "emp_II = 100 * np.sum(data[(\"path\", 'II')].to_numpy()) / 147\n",
    "\n",
    "only_III = state_array[:,2]\n",
    "atleast_III = (state_array[:,2] + state_array[:,3] + state_array[:,6] \n",
    "               + state_array[:,7] + state_array[:,10] + state_array[:,11] \n",
    "               + state_array[:,14] + state_array[:,15])\n",
    "emp_III = 100 * np.sum(data[(\"path\", 'III')].to_numpy()) / 147\n",
    "\n",
    "only_II_and_III = state_array[:,6]\n",
    "atleast_II_and_III = (state_array[:,6] + state_array[:,7] \n",
    "                      + state_array[:,14] + state_array[:,15])\n",
    "emp_II_and_III = 100 * len(data.loc[(data[(\"path\", 'II')]==1) \n",
    "                                    & (data[(\"path\", 'III')]==1)].to_numpy()) / 147\n",
    "\n",
    "only_IV = state_array[:,1]\n",
    "atleast_IV = (state_array[:,1] + state_array[:,3] + state_array[:,5] \n",
    "              + state_array[:,7] + state_array[:,9] + state_array[:,11] \n",
    "              + state_array[:,13] + state_array[:,15])\n",
    "emp_IV = 100 * np.sum(data[(\"path\", 'IV')].to_numpy()) / 147\n",
    "\n",
    "# and now for one complicated plot...\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=2*1.61), \n",
    "                 constrained_layout=True);\n",
    "spec = gs.GridSpec(ncols=3, nrows=1, figure=fig, width_ratios=[1., 1., 0.3]);\n",
    "\n",
    "# leftmost subplot\n",
    "ax = fig.add_subplot(spec[0,0])\n",
    "ax.plot(range(len(time_dist)), 100*only_II, 'o-', \n",
    "        label=r\"$\\xi_5=[0\\ 1\\ 0\\ 0]$\");\n",
    "ax.plot(range(len(time_dist)), 100*only_III, 'o-', \n",
    "        label=r\"$\\xi_3=[0\\ 0\\ 1\\ 0]$\");\n",
    "ax.plot(range(len(time_dist)), 100*only_II_and_III, 'o-', \n",
    "        label=r\"$\\xi_7=[0\\ 1\\ 1\\ 0]$\");\n",
    "ax.plot(range(len(time_dist)), 100*only_IV, 'o-', \n",
    "        label=r\"$\\xi_2=[0\\ 0\\ 0\\ 1]$\");\n",
    "ax.set_xlabel(\"time step $t$\");\n",
    "ax.set_ylim(ymax=50);\n",
    "ax.set_ylabel(\"Risk [%]\");\n",
    "ax.legend();\n",
    "\n",
    "# middle subplot\n",
    "ax = fig.add_subplot(spec[0,1])\n",
    "ax.plot(range(len(time_dist)), 100*atleast_II, 'o-', \n",
    "        label=\"lvl II involved\");\n",
    "ax.plot(range(len(time_dist)), 100*atleast_III, 'o-', \n",
    "        label=\"lvl III involved\");\n",
    "ax.plot(range(len(time_dist)), 100*atleast_II_and_III, 'o-', \n",
    "        label=\"lvl II & III involved\");\n",
    "ax.plot(range(len(time_dist)), 100*atleast_IV, 'o-', \n",
    "        label=\"lvl IV involved\");\n",
    "ax.set_xlabel(\"time step $t$\");\n",
    "ax.set_ylim(ymax=100);\n",
    "ax.legend();\n",
    "\n",
    "# rightmost subplot\n",
    "ax = fig.add_subplot(spec[0,2], sharey=ax);\n",
    "plt.setp(ax.get_yticklabels(), visible=False);\n",
    "ax.set_xticks([0, 1, 2, 3]);\n",
    "ax.set_xticklabels([\"II\", \"III\", \"II & III\", \"IV\"], rotation=-45);\n",
    "\n",
    "violin = ax.violinplot(marg_II, positions=[0]);\n",
    "violin[\"bodies\"][0].set_color(usz_blue);\n",
    "violin[\"cbars\"].set_color(usz_blue);\n",
    "ax.axhline(emp_II, color=usz_blue, ls=\"--\");\n",
    "\n",
    "violin = ax.violinplot(marg_III, positions=[1]);\n",
    "violin[\"bodies\"][0].set_color(usz_orange);\n",
    "violin[\"cbars\"].set_color(usz_orange);\n",
    "ax.axhline(emp_III, color=usz_orange, ls=\"--\");\n",
    "\n",
    "violin = ax.violinplot(marg_II_and_III, positions=[2]);\n",
    "violin[\"bodies\"][0].set_color(usz_red);\n",
    "violin[\"cbars\"].set_color(usz_red);\n",
    "ax.axhline(emp_II_and_III, color=usz_red, ls=\"--\");\n",
    "\n",
    "violin = ax.violinplot(marg_IV, positions=[3]);\n",
    "violin[\"bodies\"][0].set_color(usz_green);\n",
    "violin[\"cbars\"].set_color(usz_green);\n",
    "ax.axhline(emp_IV, color=usz_green, ls=\"--\");\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_evolution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_evolution.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 3:__ (left) Probability of certain hidden state vs time; (middle) Probability of LNLâ€™s involvement marginalized over the other LNLâ€™s involvement vs time; (right) The same probabilities as in the middle, but also marginalized over the time-prior and depicted as violin plots. The dashed lines represent the prevalence in the dataset8 that was used for training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=1.8), \n",
    "                 constrained_layout=True)\n",
    "spec = gs.GridSpec(ncols=2, nrows=1, figure=fig, \n",
    "                   width_ratios=[1., 0.35], height_ratios=[1.])\n",
    "\n",
    "ax_im = fig.add_subplot(spec[0,0])\n",
    "ax_im.set_title(\"Probabilities [%] of states at different time steps\")\n",
    "ax_im.imshow(state_array, cmap=green_to_red);\n",
    "for i in range(len(time_dist)):\n",
    "    for j in range(len(model.state_list)):\n",
    "        if np.around(state_array[i,j]*100,1) >= 1.:\n",
    "            ax_im.text(j,i, f\"{state_array[i,j]*100:.1f}\", \n",
    "                       ha=\"center\", va=\"center\", \n",
    "                       color=\"white\", fontsize=\"xx-small\")\n",
    "ax_im.set_xticks(range(len(model.state_list)))\n",
    "ax_im.set_xticklabels(model.state_list, rotation=45);\n",
    "ax_im.set_ylabel(\"time step $t$\")\n",
    "ax_im.set_xlabel(r\"state $\\xi$\")\n",
    "ax_im.grid(False)\n",
    "\n",
    "ax_pr = fig.add_subplot(spec[0,1], sharey=ax_im)\n",
    "ax_pr.set_title(\"Time prior (PDF)\")\n",
    "ax_pr.plot(time_dist, range(len(time_dist)), \"o-\")\n",
    "plt.setp(ax_pr.get_yticklabels(), visible=False);\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_evo_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_evo_matrix.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 4:__ Probability of being in each hidden state as a function of time (left). The color indicates low (green) and high (red) probabilities, which are also written on the respective pixel in percent if larger than 1%. We used the mean of the inferred parameter samples to compute the probabilities. On the right, the used time-prior is plotted with which each column on the left will be weighted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Cross-)Validation\n",
    "\n",
    "### Comparison of risk & prevalences\n",
    "\n",
    "As an attempt to validate the model with the limited data we have, I'll start by simply comparing the prevalence of certain patterns of involvement to the prediction of the model for the respective state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin = 100\n",
    "np.random.seed(SEED)\n",
    "\n",
    "risks = np.zeros(shape=(len(model.obs_list), len(samples_HMM[::thin])), dtype=float)\n",
    "\n",
    "for i, sample in enumerate(np.random.permutation(samples_HMM[::thin])):\n",
    "    for j, obs in enumerate(model.obs_list):\n",
    "        model.spread_probs = sample\n",
    "        risks[j,i] = model.risk(\n",
    "            inv=obs, diagnoses={\"path\": [None, None, None, None]}, \n",
    "            time_dist=time_dists[\"early\"], mode=\"HMM\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences, _ = lymph.utils.comp_state_dist(data[\"path\"].values)\n",
    "\n",
    "validation_df = pd.DataFrame({\"state\": [str(obs) for obs in model.obs_list], \n",
    "                              \"occurence\": occurences, \n",
    "                              \"percentage\": 100 * occurences / np.sum(occurences), \n",
    "                              \"prediction\": 100 * np.mean(risks, axis=1)})\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Table 2:__ Prevalence of each state in the dataset (column \"percentage\") and the corresponding prediction from the model (column \"prediction\").*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-fold cross-validation\n",
    "\n",
    "Now I will split the dataset randomly into three equally large parts. Then I will train the model on all three combinations of two of these thirds and compare them to the respectively remaining third to see if the results are still plausible.\n",
    "\n",
    "These results are actually not in our paper, since they were already done in a similar fashion by Pouymayou et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = []\n",
    "\n",
    "# first third\n",
    "subsets.append(data.sample(frac=1./3.))\n",
    "rest = data.drop(subsets[0].index)\n",
    "\n",
    "# second third\n",
    "subsets.append(rest.sample(frac=0.5))\n",
    "\n",
    "# third third\n",
    "subsets.append(rest.drop(subsets[1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,subset in enumerate(subsets):\n",
    "    subset.to_csv(f\"./data/cross-validation_set{i+1}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the sampler\n",
    "cross_validation_samples = []\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if DRAW_SAMPLES:\n",
    "    for i,subset in enumerate(subsets):\n",
    "        # load data subset (or rather the remainder)\n",
    "        model.patient_data = data.drop(subset.index)\n",
    "\n",
    "        # starting point\n",
    "        theta0 = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(nwalker, ndim, model.log_likelihood, \n",
    "                                            args=[[\"early\"], time_dists], \n",
    "                                            moves=moves, pool=pool)\n",
    "            sampler.run_mcmc(theta0, nstep, progress=True)\n",
    "\n",
    "        cross_validation_samples.append(\n",
    "            sampler.get_chain(flat=True, discard=burnin)\n",
    "        )\n",
    "        \n",
    "        # saving the sampled data to disk for later convenience\n",
    "        np.save(f\"./samples/cross-validation-samples{i+1}.npy\", \n",
    "                cross_validation_samples[i])\n",
    "    \n",
    "else:\n",
    "    for i,subset in enumerate(subsets):\n",
    "        # loading in case we don't want to draw all the samples again\n",
    "        cross_validation_samples.append(\n",
    "            np.load(f\"./samples/cross-validation-samples{i+1}.npy\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systm.load_data(data, t_stages=[\"early\"],\n",
    "                modality_spsn=modality_spsn, mode=\"HMM\",\n",
    "                gen_C_kwargs={\"delete_ones\": False})\n",
    "\n",
    "C_total = systm.C_dict[\"early\"]\n",
    "f_total = systm.f_dict[\"early\"]\n",
    "\n",
    "C_matrices = []\n",
    "f_vectors = []\n",
    "\n",
    "for i,subset in enumerate(subsets):\n",
    "    systm.load_data(subset, t_stages=[\"early\"],\n",
    "                    modality_spsn=modality_spsn, mode=\"HMM\",\n",
    "                    gen_C_kwargs={\"delete_ones\": False})\n",
    "    C_matrices.append(systm.C_dict[\"early\"])\n",
    "    f_vectors.append(systm.f_dict[\"early\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = pd.DataFrame(columns=[\"subset 1\", \"subset 2\", \"subset 3\", \"total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through all the states\n",
    "for s in range(16):\n",
    "    new_row = {}\n",
    "    try:\n",
    "        idx = np.where(C_total[s,:])[0][0]\n",
    "        new_row[\"total\"] = f_total[idx]\n",
    "    except IndexError:\n",
    "        new_row[\"total\"] = 0\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            idx = np.where(C_matrices[i][s,:])[0][0]\n",
    "            new_row[f\"subset {i+1}\"] = f_vectors[i][idx]\n",
    "        except IndexError:\n",
    "            new_row[f\"subset {i+1}\"] = 0\n",
    "        \n",
    "    cross_val = cross_val.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_spsn_dict = {\"PET\": [0.86, 0.79]}\n",
    "systm.modalities = obs_spsn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin = 500\n",
    "state_list = systm.state_list\n",
    "risk_matrix = np.zeros(shape=(16,4,int(samples_HMM.shape[0]/thin)), dtype=float)\n",
    "\n",
    "for s,state in enumerate(state_list):\n",
    "    for i,sample in enumerate([cross_validation_samples[0], \n",
    "                               cross_validation_samples[1], \n",
    "                               cross_validation_samples[2], \n",
    "                               samples_HMM]):\n",
    "        sample_set = np.random.permutation(sample)[::thin]\n",
    "        risk_matrix[s,i] = [\n",
    "            systm.risk(\n",
    "                val, inv=state, \n",
    "                time_dist=time_dists[\"early\"]) for val in sample_set\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the most interesting states (others have very small probabilities)\n",
    "row_selection = [0, 4, 6, 7, 14]\n",
    "titles = [\"subset 1\", \"subset 2\", \"subset 3\", \"whole dataset\"]\n",
    "\n",
    "fig, ax = plt.subplots(len(row_selection),4, figsize=(8,6), \n",
    "                       sharex=\"col\", sharey=\"row\")\n",
    "x = np.linspace(0., 1., 100)\n",
    "\n",
    "for s,row in enumerate(row_selection):\n",
    "    for i,subset in enumerate([*subsets, data]):\n",
    "        if s == 0:\n",
    "            ax[s,i].set_title(titles[i])\n",
    "        if i == 0:\n",
    "            ax[s,i].set_ylabel(f\"$\\\\xi_{{{row+1}}}=$\\n{state_list[row]}\")\n",
    "        \n",
    "        ax[s,i].plot(x, sp.stats.beta.pdf(x, \n",
    "                                          a=cross_val.iloc[row,i], \n",
    "                                          b=len(subset)-cross_val.iloc[row,i]), \n",
    "                     color=usz_orange, linewidth=2)\n",
    "        ax[s,i].hist(risk_matrix[row,i], bins=10, density=True, \n",
    "                     histtype=\"stepfilled\", color=usz_blue)\n",
    "        ax[s,i].set_xlim([0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 5:__ Histograms over predicted risk of certain states (blue) compared to the Beta distribution over the same risk, resulting from the prevalence of the respective state in the dataset (orange). This is plotted for the three subsets of the 3-fold cross-validation as well as the whole dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Bayesian Network\n",
    "\n",
    "To be able to compare our results to the Bayesian network by [Pouymayou et al.](#pouymayou) we needed to recreate it using the same sampler. To this end, the `lymph` package also supports computing the Bayesian network likelihood for a given graph and observational modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {('tumor', 'primary')  : ['I', 'II', 'III', 'IV'], \n",
    "         ('lnl', 'I')          : ['II'], \n",
    "         ('lnl', 'II')         : ['III'], \n",
    "         ('lnl', 'III')        : ['IV'], \n",
    "         ('lnl', 'IV')         : []}\n",
    "\n",
    "model = lymph.System(graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning (BN)\n",
    "\n",
    "All that is different to the learning round [above](#Learning-(HMM)) is that one has to specify the `mode` to be `\"BN\"` instead of `\"HMM\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define specificity and sensitivity for diagnostic modalities\n",
    "modality_spsn = {\"path\": [1., 1.]}\n",
    "\n",
    "model.modalities = modality_spsn\n",
    "\n",
    "# generate C matrix from data\n",
    "model.load_data(data, mode=\"BN\")\n",
    "\n",
    "# check if likelihood works\n",
    "llh = model.log_likelihood(np.random.uniform(size=(7,)), mode=\"BN\")\n",
    "print(llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the sampler\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "if DRAW_SAMPLES:\n",
    "    # starting point\n",
    "    np.random.seed(SEED)\n",
    "    initial_spread_probs = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "\n",
    "    # the actual sampling round\n",
    "    if __name__ == \"__main__\":\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(\n",
    "                nwalker, ndim, \n",
    "                model.log_likelihood, kwargs={\"mode\": \"BN\"},\n",
    "                moves=moves, pool=pool\n",
    "            )\n",
    "            sampler.run_mcmc(initial_spread_probs, nstep, progress=True)\n",
    "\n",
    "    # extracting 200,000 of the 400,000 samples\n",
    "    samples_BN = sampler.get_chain(flat=True, discard=burnin)\n",
    "\n",
    "    # saving the sampled data to disk for later convenience\n",
    "    np.save(\"./samples/BN.npy\", samples_BN)\n",
    "    \n",
    "else:\n",
    "    # loading in case we don't want to draw all the samples again\n",
    "    samples_BN = np.load(\"./samples/BN.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRAW_SAMPLES:\n",
    "    # check acceptance faction of the sampler to get an indication on whether sth\n",
    "    # went wrong or not\n",
    "    ar = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"the BN sampler accepted {ar * 100 :.2f} % of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r\"$b_1$\", r\"$b_2$\", r\"$b_3$\", r\"$b_4$\", \n",
    "          r\"$t_{12}$\", r\"$t_{23}$\", r\"$t_{34}$\"]\n",
    "\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=1))\n",
    "\n",
    "# using the corner plot package\n",
    "corner.corner(samples_BN, labels=labels, smooth=True, fig=fig, \n",
    "              hist_kwargs={'histtype': 'stepfilled', 'color': usz_green}, \n",
    "              **{'plot_datapoints': False, 'no_fill_contours': True, \n",
    "                 \"density_cmap\": white_to_green.reversed(), \n",
    "                 \"contour_kwargs\": {\"colors\": \"k\"}, \n",
    "                 \"levels\": np.array([0.2, 0.5, 0.8])}, \n",
    "              show_titles=True, title_kwargs={\"fontsize\": \"medium\"});\n",
    "\n",
    "axes = fig.get_axes()\n",
    "for ax in axes:\n",
    "    ax.grid(False)\n",
    "    \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/corner_BN.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/corner_BN.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 6:__ Corner plor of sampled parameters using the Bayesian network model. Qualitatively, this looks very similar to Figure 2, but since the HMM model works with rates instead of absolute probabilities, the values here are larger.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Predictions\n",
    "\n",
    "We can now compute distributions over risks using both the HMM model, as well as the BN.\n",
    "\n",
    "### Evolving beyond \"early\" T-stage\n",
    "\n",
    "We can use the parameters inferred from the early T-stage dataset and use time priors that expect to see a patient's diagnose later on to estimate how risks of involvement might increase over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {('tumor', 'primary')  : ['I', 'II', 'III', 'IV'], \n",
    "         ('lnl'  , 'I')        : ['II'], \n",
    "         ('lnl'  , 'II')       : ['III'], \n",
    "         ('lnl'  , 'III')      : ['IV'], \n",
    "         ('lnl'  , 'IV' )      : []}\n",
    "\n",
    "tst_model = lymph.System(graph=graph)\n",
    "\n",
    "# set specificity & sensitivity of diagnostic modality (here CT) manually\n",
    "ct_spsn = {\"CT\": [0.76, 0.81]}\n",
    "tst_model.modalities = ct_spsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time priors\n",
    "time_dists = {}\n",
    "time_dists['early'] = sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, 0.4)\n",
    "time_dists['mid'] = sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, 0.55)\n",
    "time_dists['late'] = sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, 0.7)\n",
    "\n",
    "# what do we want to know, what do we know?\n",
    "inv = np.array([None, None, 1, None])  # we're interested in the risk of LNL 3\n",
    "# our observation is that lvl 2 is involved\n",
    "diagnoses = {\"CT\": np.array([0, 1, 0, 0])}\n",
    "\n",
    "thin = 50\n",
    "# risk for HMM and different \"T-stages\"\n",
    "early = []\n",
    "mid = []\n",
    "late = []\n",
    "np.random.seed(SEED)\n",
    "for sample in np.random.permutation(samples_HMM)[::thin]:\n",
    "    tst_model.spread_probs = sample\n",
    "    early.append(\n",
    "        tst_model.risk(\n",
    "            inv=inv, diagnoses=diagnoses, \n",
    "            time_dist=time_dists[\"early\"], \n",
    "            mode=\"HMM\"\n",
    "        )\n",
    "    )\n",
    "    mid.append(\n",
    "        tst_model.risk(\n",
    "            inv=inv, diagnoses=diagnoses, \n",
    "            time_dist=time_dists[\"mid\"], \n",
    "            mode=\"HMM\"\n",
    "        )\n",
    "    )\n",
    "    late.append(\n",
    "        tst_model.risk(\n",
    "            inv=inv, diagnoses=diagnoses, \n",
    "            time_dist=time_dists[\"late\"], \n",
    "            mode=\"HMM\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# risk for BN\n",
    "bn = []\n",
    "np.random.seed(SEED)\n",
    "for sample in np.random.permutation(samples_BN)[::thin]:\n",
    "    tst_model.spread_probs = sample\n",
    "    bn.append(tst_model.risk(inv=inv, diagnoses=diagnoses, mode=\"BN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 35, 50)\n",
    "r = (0, 35)\n",
    "fig, ax = plt.subplots(figsize=set_size())\n",
    "\n",
    "ax.hist(np.asarray(early)*100., bins=bins, density=True, \n",
    "        histtype=\"stepfilled\", color=usz_green, label=\"$p = 0.4$\");\n",
    "ax.hist(np.asarray(mid)*100., bins=bins, density=True, alpha=0.8, \n",
    "        histtype=\"stepfilled\", color=usz_orange, label=\"$p = 0.55$\");\n",
    "ax.hist(np.asarray(late)*100., bins=bins, density=True, alpha=0.8, \n",
    "        histtype=\"stepfilled\", color=usz_red, label=\"$p = 0.7$\");\n",
    "ax.hist(np.asarray(bn)*100., bins=bins, histtype=\"step\", density=True, \n",
    "        color=usz_blue, label=\"Bayesian network\");\n",
    "\n",
    "ax.set_xlim(r)\n",
    "ax.set_xlabel(\"risk $R$ [%]\");\n",
    "ax.set_ylabel(r\"$p(R)$\");\n",
    "ax.tick_params();\n",
    "ax.legend();\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_risk_increaseP.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_risk_increaseP.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 7:__ Risk prediction for LNL III, given observed positive involvement in LNL II and negative observations in all other LNLs (assuming $s_N = 81\\%$ and $s_P = 76\\%$). The Binomial parameter p was fixed to 0.4 for parameter learning (green), representing early T-category patients. Increasing this parameter results in higher risk. The blue outline shows the risk in level III obtained for the Bayesian network model. The histograms correspond to 1% of the 200,000 samples.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to [Pouymayou et al.](#pouymayou)\n",
    "\n",
    "Now we will compare how the sampled HMM's, sampled BN's and maximum likelihood BN's risk predictions compare to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "thin = 50\n",
    "\n",
    "# what do we want to know?\n",
    "inv = np.array([[1   , None, None, None],\n",
    "                [None, 1   , None, None],\n",
    "                [None, None, 1   , None],\n",
    "                [None, None, None, 1   ]])\n",
    "\n",
    "# what do we know?\n",
    "obs = np.array([[0, 0, 0, 0],\n",
    "                [0, 1, 0, 0],\n",
    "                [0, 1, 1, 0],\n",
    "                [0, 0, 1, 0]])\n",
    "\n",
    "# risk for HMM and two different \"T-stages\" (early and late)\n",
    "np.random.seed(SEED)\n",
    "hmm_risk = np.zeros(shape=(4,4,(nstep-burnin)*nwalker//thin))\n",
    "for i, sample in enumerate(np.random.permutation(samples_HMM)[::thin]):\n",
    "    tst_model.spread_probs = sample\n",
    "    for k in range(4):\n",
    "        for l in range(4):\n",
    "            hmm_risk[k,l,i] = tst_model.risk(\n",
    "                inv=inv[k], diagnoses={\"CT\": obs[l]}, \n",
    "                time_dist=time_dists[\"early\"],  mode=\"HMM\"\n",
    "            )\n",
    "\n",
    "# risk for BN\n",
    "np.random.seed(SEED)\n",
    "bn_risk = np.zeros(shape=(4,4,(nstep-burnin)*nwalker//thin))\n",
    "for i, sample in enumerate(np.random.permutation(samples_BN)[::thin]):\n",
    "    tst_model.spread_probs = sample\n",
    "    for k in range(4):\n",
    "        for l in range(4):\n",
    "            bn_risk[k,l,i] = tst_model.risk(\n",
    "                inv=inv[k], diagnoses={\"CT\": obs[l]}, mode=\"BN\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=set_size(width=\"full\"), constrained_layout=True)\n",
    "spec = gs.GridSpec(ncols=4, nrows=4, figure=fig)\n",
    "\n",
    "lvls = [\"I\", \"II\", \"III\", \"IV\"]\n",
    "txt = [\"Ã˜\", \"II\", \"II & III\", \"III\"]\n",
    "risk_map = LinearSegmentedColormap.from_list(\"risk_map\", [usz_green, \n",
    "                                                          usz_gray, \n",
    "                                                          usz_red, \n",
    "                                                          usz_red], N=256)\n",
    "n_bins = 25\n",
    "\n",
    "for i in range(4):\n",
    "    if (i == 0) or (i == 3):\n",
    "        bins = np.linspace(0., 12., 30)\n",
    "        for j in range(4):\n",
    "            if j == 0:\n",
    "                ax = fig.add_subplot(spec[i,j])\n",
    "                ax.set_ylabel(f\"{lvls[i]}\");\n",
    "            else:\n",
    "                ax = fig.add_subplot(spec[i,j], sharey=ax)\n",
    "                plt.setp(ax.get_yticklabels(), visible=False)\n",
    "                \n",
    "            ax.set_xlim(bins[0], bins[-1])\n",
    "            \n",
    "            tmp_mean = np.mean(hmm_risk[i,j])\n",
    "            hmm_color = risk_map(tmp_mean)\n",
    "            ax.axvline(pouymayou_MLrisk[i,j]*100., \n",
    "                       color=usz_orange, label=\"Pouymayou et al\");\n",
    "            _, bins, _ = ax.hist(hmm_risk[i,j]*100., bins=bins, \n",
    "                                 histtype=\"stepfilled\", density=True, \n",
    "                                 color=hmm_color) #, label=\"HMM sampling\");\n",
    "            ax.hist(bn_risk[i,j]*100., bins=bins, density=True, \n",
    "                    histtype=\"step\", label=\"BN sampling\", color=usz_blue);\n",
    "            ax.tick_params(labelsize=\"xx-small\")\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_title(f\"{txt[j]}\", \n",
    "                             fontsize=\"medium\", fontweight=\"normal\");\n",
    "            else:\n",
    "                ax.set_xlabel(\"risk [%]\");\n",
    "                \n",
    "    else:\n",
    "        ax = fig.add_subplot(spec[i,:])\n",
    "        bins = np.linspace(0., 100., 150)\n",
    "        ax.set_xlim(bins[0], bins[-1])\n",
    "        ax.set_ylabel(f\"{lvls[i]}\");\n",
    "        \n",
    "        for j in range(4):\n",
    "            tmp_mean = np.mean(hmm_risk[i,j])\n",
    "            hmm_color = risk_map(tmp_mean)\n",
    "            ax.axvline(pouymayou_MLrisk[i,j]*100., \n",
    "                       color=usz_orange, label=\"Pouymayou et al\");\n",
    "            n, bins, _ = ax.hist(hmm_risk[i,j]*100., bins=bins, \n",
    "                                 histtype=\"stepfilled\", density=True, \n",
    "                                 color=hmm_color) #, label=\"HMM sampling\");\n",
    "            ax.hist(bn_risk[i,j]*100., bins=bins, density=True, \n",
    "                    histtype=\"step\", label=\"BN sampling\", color=usz_blue, \n",
    "                    linestyle='-');\n",
    "            ax.set_xticks(np.linspace(0,100, 11))\n",
    "            ax.tick_params(labelsize=\"xx-small\")\n",
    "            \n",
    "            if ((i == 2) and (j == 0)):\n",
    "                ax.text(x=100*pouymayou_MLrisk[i,j]-4*(bins[1]-bins[0]), \n",
    "                        y=0.35, \n",
    "                        s=txt[j])\n",
    "            elif ((i == 1) and (j == 2)):\n",
    "                ax.text(x=100*pouymayou_MLrisk[i,j]+2.5*(bins[1]-bins[0]), \n",
    "                        y=0.2, \n",
    "                        s=txt[j])\n",
    "            else:\n",
    "                ax.text(x=100*pouymayou_MLrisk[i,j]+(bins[1]-bins[0]), \n",
    "                        y=np.max(n)+0.05, \n",
    "                        s=txt[j])\n",
    "                \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/HMM_BN_risk_comparison.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/HMM_BN_risk_comparison.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 8:__ Risk assessment for the involvement of different LNLs (rows), given positive observational findings in specified LNLs (columns or labels next to histograms). E.g. row 3 depicts the risk of involvement in LNL III, given different observed involvements (from left to right: no involvement, LNL II only, LNL III only, and LNL II and III but no others). The orange line depicts the maximum likelihood result from [[2]](#pouymayou), the blue outline histogram represents the BN sampling solutions and the solid coloured histograms are the results from the HMM. The colour goes from green (low risk) to red (high risk). Of 200,000 parameter samples, 2% were used to create this plot.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultaneous Learning\n",
    "If we learn both the system's parameters AND the center of the time prior at the same time. But the naive way just leads to overfitting and very unrealistic combinations of parameters. So, what we are doing here is fixing the time prior for the early stage learning (and use the sanguineti data with chosen N0-ratio) and learn the probability rates along with the time-prior for the late stage (where the data only consists of N0 / N+ patients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {('tumor', 'primary')  : ['I', 'II', 'III', 'IV'], \n",
    "         ('lnl', 'I')          : ['II'], \n",
    "         ('lnl', 'II')         : ['III'], \n",
    "         ('lnl', 'III')        : ['IV'], \n",
    "         ('lnl', 'IV')         : []}\n",
    "\n",
    "model = lymph.System(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define specificity and sensitivity for diagnostic modalities\n",
    "pathology_spsn = {\"path\": [1., 1.]}\n",
    "\n",
    "model.modalities = pathology_spsn\n",
    "model.patient_data = data\n",
    "\n",
    "# create late T-stage diagnose matrices\n",
    "n_late = len(data)\n",
    "shape = (len(model.state_list), n_late)\n",
    "model._diagnose_matrices[\"late\"] = np.zeros(shape=shape)\n",
    "\n",
    "late_n0_num_patients = int(n_late * (1 - (1. / 1.2)))\n",
    "late_nplus_num_patients = int(n_late / 1.2)\n",
    "\n",
    "# N0 patients\n",
    "model._diagnose_matrices[\"late\"][0 , :late_n0_num_patients] = 1.\n",
    "model._diagnose_matrices[\"late\"][1:, late_n0_num_patients:] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.diagnose_matrices[\"late\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define likelihood function for simultaneous learning\n",
    "def simultaneous_log_likelihood(theta):\n",
    "    len_spread_probs = len(model.spread_probs)\n",
    "    spread_probs = theta[:len_spread_probs]\n",
    "    early_p = 0.4\n",
    "    late_p = theta[-1]\n",
    "    \n",
    "    if late_p > 1. or late_p < 0.:\n",
    "        return -np.inf\n",
    "    \n",
    "    t = np.arange(MAX_T + 1)\n",
    "    pt = lambda p : sp.stats.binom.pmf(t, MAX_T, p)\n",
    "    \n",
    "    time_dists = {}\n",
    "    time_dists[\"early\"] = pt(early_p)\n",
    "    time_dists[\"late\"] = pt(late_p)\n",
    "    \n",
    "    return model.marginal_log_likelihood(\n",
    "        spread_probs, [\"early\", \"late\"], time_dists=time_dists\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.random.uniform(size=(8,))\n",
    "llh = simultaneous_log_likelihood(theta0)\n",
    "print(llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim, nwalker, nstep, burnin = 7 + 1, 200, 2000, 1000\n",
    "np.random.seed(SEED)\n",
    "theta0 = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "if DRAW_SAMPLES:\n",
    "    if __name__ == \"__main__\":\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(\n",
    "                nwalker, ndim, \n",
    "                simultaneous_log_likelihood, \n",
    "                moves=moves, pool=pool\n",
    "            )\n",
    "            sampler.run_mcmc(theta0, nstep, progress=True)\n",
    "\n",
    "    # extracting 200,000 of the 400,000 samples\n",
    "    samples_simultaneous = sampler.get_chain(flat=True, discard=burnin)\n",
    "\n",
    "    # saving the sampled data to disk for later convenience\n",
    "    np.save(\"./samples/simultaneous.npy\", samples_simultaneous)\n",
    "    \n",
    "else:\n",
    "    # loading in case we don't want to draw all the samples again\n",
    "    samples_simultaneous = np.load(\"./samples/simultaneous.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DRAW_SAMPLES:\n",
    "    # check acceptance faction of the sampler to get an indication on whether sth\n",
    "    # went wrong or not\n",
    "    ar = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"the simultaneous sampler accepted {ar * 100 :.2f} % of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r\"$\\tilde{b}_1$\", r\"$\\tilde{b}_2$\", \n",
    "          r\"$\\tilde{b}_3$\", r\"$\\tilde{b}_4$\", \n",
    "          r\"$\\tilde{t}_{12}$\", r\"$\\tilde{t}_{23}$\", \n",
    "          r\"$\\tilde{t}_{34}$\", r\"$p$\"]\n",
    "\n",
    "fig = plt.figure(figsize=set_size(width=\"full\", ratio=1))\n",
    "corner.corner(samples_simultaneous, labels=labels, smooth=True, fig=fig, \n",
    "              hist_kwargs={'histtype': 'stepfilled', 'color': usz_blue}, \n",
    "              **{'plot_datapoints': False, 'no_fill_contours': True, \n",
    "                 \"density_cmap\": white_to_blue.reversed(), \n",
    "                 \"contour_kwargs\": {\"colors\": \"k\"}, \n",
    "                 \"levels\": np.array([0.2, 0.5, 0.8])}, \n",
    "              show_titles=True, title_kwargs={\"fontsize\": \"medium\"});\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/corner_simultaneous.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/corner_simultaneous.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 9:__ Corner plot of the sampled paramters during the simultaneous sampling process, were we also inferred the p-parameter of the late T-categorie's time-prior.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_late_p = np.mean(samples_simultaneous[:,7])\n",
    "\n",
    "fig, ax = plt.subplots(1,2, \n",
    "                       figsize=set_size(width=\"full\", ratio=1.61**2), \n",
    "                       constrained_layout=True)\n",
    "\n",
    "ax[0].axvline(0.4, color=usz_blue, linewidth=2, \n",
    "              label=r\"$p_{\\mathrm{early}}$ (fixed)\");\n",
    "ax[0].hist(samples_simultaneous[:,7], bins=40, density=True, \n",
    "           color=usz_red, histtype=\"stepfilled\", \n",
    "           label=r\"$p_{\\mathrm{late}}$\");\n",
    "ax[0].set_xlabel(\"Binomial parameter $p$\");\n",
    "# ax[0].set_ylabel(r\"$p\\left(\\theta_p^T\\right)$\", fontsize=\"large\");\n",
    "ax[0].legend();\n",
    "\n",
    "t = np.arange(MAX_T+1)\n",
    "dist_sum = np.zeros_like(t, dtype=float)\n",
    "\n",
    "for sample in np.random.permutation(samples_simultaneous[::100]):\n",
    "    p = sample[-1]\n",
    "    dist_sum += sp.stats.binom.pmf(t, MAX_T, p)\n",
    "\n",
    "dist_avrg = dist_sum / len(samples_simultaneous[::100])\n",
    "\n",
    "ax[1].plot(t, sp.stats.binom.pmf(t, MAX_T, 0.4), 'o-', \n",
    "           label=r\"$p_{\\mathrm{early}}$ (fixed)\");\n",
    "ax[1].plot(t, sp.stats.binom.pmf(t, MAX_T, mean_late_p), 'o-', color=usz_red, \n",
    "           label=r\"$\\mathbb{E}[p_{\\mathrm{late}}]$\");\n",
    "ax[1].plot(t, dist_avrg, 'o--', color=usz_red, alpha=0.5, \n",
    "           label=\"avergaged binomials\");\n",
    "\n",
    "ax[1].legend();\n",
    "ax[1].set_xlabel(\"Time step $t$\");\n",
    "ax[1].set_ylabel(r\"$p_T(t)$\");\n",
    "ax[1].set_xlim([1,MAX_T]);\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/simultaneous_learnedP.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/simultaneous_learnedP.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 10:__ Sampled late T-category p parameter given an early T-category cohort and a fixed fraction of N0 patients (20%) for late T-category (left). Plots of the PMFs of the fixed early T-category binomial distribution, the distribution for the expected value of the late T-category parameter as well as the average distribution resulting from the many sampled binomials (right).*\n",
    "\n",
    "Now we can again compare risk predictions for different T-stages of disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {('tumor', 'primary')  : ['I', 'II', 'III', 'IV'], \n",
    "         ('lnl'  , 'I')        : ['II'], \n",
    "         ('lnl'  , 'II')       : ['III'], \n",
    "         ('lnl'  , 'III')      : ['IV'], \n",
    "         ('lnl'  , 'IV' )      : []}\n",
    "\n",
    "tst_systm = lymph.System(graph=graph)\n",
    "tst_systm.modalities = {\"CT\": [0.76, 0.81]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "subset = np.random.permutation(samples_simultaneous[::20])\n",
    "nsubset = len(subset)\n",
    "risk_III = np.zeros(shape=(4, nsubset))\n",
    "risk_IV = np.zeros(shape=(4, nsubset))\n",
    "\n",
    "inv_III = np.array([None, None, 1, None])\n",
    "obs_III = np.array([[0, 0, 0, 0],   # no involvement in lvl II observed\n",
    "                    [0, 1, 0, 0]])  # involvement observed\n",
    "\n",
    "inv_IV = np.array([None, None, None, 1])\n",
    "obs_IV = np.array([[0, 0, 0, 0], \n",
    "                   [0, 1, 1, 0]])\n",
    "\n",
    "for i, th in enumerate(subset):\n",
    "    tst_systm.spread_probs = th[:7]\n",
    "    prior = np.vstack([sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, 0.4), \n",
    "                       sp.stats.binom.pmf(np.arange(MAX_T+1), MAX_T, th[7])])\n",
    "    \n",
    "    for k in range(4):\n",
    "        risk_III[k,i] = tst_systm.risk(\n",
    "            inv=inv_III, diagnoses={\"CT\": obs_III[k % 2]}, \n",
    "            time_dist=prior[k // 2]\n",
    "        )\n",
    "        risk_IV[k,i] = tst_systm.risk(\n",
    "            inv=inv_IV, diagnoses={\"CT\": obs_IV[k % 2]}, \n",
    "            time_dist=prior[k // 2]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, \n",
    "                       figsize=set_size(width=\"full\", ratio=2*1.5), \n",
    "                       constrained_layout=True)\n",
    "kwargs = [{\"histtype\": \"stepfilled\", \"alpha\": 0.5}, \n",
    "          {\"histtype\": \"step\", \"linewidth\": 1.5}]\n",
    "colors = [usz_blue, usz_orange]\n",
    "time_label = [\"early\", \"late\"]\n",
    "inv_III_label = [\"no observed involvement\", \n",
    "                 \"only LNL II observed involved\"]\n",
    "inv_IV_label = [\"no observed involvement\", \n",
    "                \"LNL II & III observed involved\"]\n",
    "\n",
    "for k in range(4):\n",
    "    bins = np.linspace(0., 25., 50)\n",
    "    ax[0].hist(100*risk_III[k], bins=bins, density=True, \n",
    "               color=colors[k // 2], **kwargs[k % 2], \n",
    "               label=f\"{time_label[k // 2]} | {inv_III_label[k % 2]}\")\n",
    "    ax[0].set_xlabel(\"risk $R$ [%]\");\n",
    "    ax[0].set_ylabel(r\"$p(R)$\");\n",
    "    ax[0].set_xlim([bins[0], bins[-1]])\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"III\")\n",
    "    \n",
    "    bins = np.linspace(0., 15., 50)\n",
    "    ax[1].hist(100*risk_IV[k], bins=bins, density=True, \n",
    "               color=colors[k // 2], **kwargs[k % 2], \n",
    "               label=f\"{time_label[k // 2]} | {inv_IV_label[k % 2]}\")\n",
    "    ax[1].set_xlabel(\"risk $R$ [%]\");\n",
    "    ax[1].set_xlim([bins[0], bins[-1]]);\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(\"IV\")\n",
    "    \n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/simultaneous_risk.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/simultaneous_risk.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *__Figure 11:__ Distributions over risk of involvement for LNL III (left) and LNL IV (right), each for early and late T-category as well as depending on the given observed involvement. The sampled parameters displayed here are a randomly selected subset (1% of 200,000) from simultaneous learning. Comparison with Fig. 8 shows that these predictions still agree with the results from the early stage only learning.*\n",
    "\n",
    "## References\n",
    "1. <a class=\"anchor\" id=\"sanguineti\"></a>Sanguineti Giuseppe [et al.] Defining the risk of involvement for each neck nodal level in patients with early T-stage node-positive oropharyngeal carcinoma. [Journal] // International Journal of Radiation Oncology Biology Physics. - 2008. - 5 : Vol. 74. - pp. 1356-1364.\n",
    "2. <a class=\"anchor\" id=\"pouymayou\"></a>Pouymayou Bertrand [et al.] A Bayesian network model of lymphatic tumor progression for personalized elective CTV definition in head and neck cancers [Journal] // Physics in Medicine and Biology. - 2019. - 16 : Vol. 64. - p. 165003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### t-Dependence of Rates\n",
    "\n",
    "Since the HMM-formalism has more parameters than the BN through its time prior, we expect the system to be somewhat overdetermined. In our case this means that we can basically choose an arbitrary number of time steps and the base and transition probability rates will essentially adapt to our choice. To see *how* the rates depend on the time prior's length we'll look at a simplistic example:\n",
    "\n",
    "For the simplest example, the time prior $p_T(t,T)=1 / T$ is uniform and we're only looking at a system with one node that has empirically an involvement probability of $p^*$ (e.g. $0.4$), the base probability rate $p$ must become smaller, as the length of the uniform prior increases. More formally,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p^* \n",
    "&= \\sum_{t=1}^{T}{\\frac{1}{T}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{bmatrix} 1-p & p \\\\ 0 & 1 \\end{bmatrix}^t\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}} \\\\\n",
    "&= \\sum_{t=1}^{T}{\\frac{1}{T}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{bmatrix} (1-p)^t & 1-(1-p)^t \\\\ 0 & 1 \\end{bmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}} \\\\\n",
    "&= \\sum_{t=1}^{T}{\\frac{1}{T}\\sum_{k=1}^{t}{{t\\choose k}p^k(1-p)^{t-k}}}\n",
    "= \\sum_{t=1}^{T}{\\frac{1}{T}\\left[ 1 - (1-p)^t \\right]} \\\\\n",
    "&= 1 - \\frac{1}{T}\\sum_{t=1}^{T}{(1-p)^t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let's write $q = 1 - p$ and then the sum as\n",
    "\n",
    "$$\n",
    "s = \\sum_{t=1}^{T}{q^t} = q + q^2 + \\ldots + q^T\n",
    "$$\n",
    "\n",
    "hence\n",
    "\n",
    "$$\n",
    "q\\cdot s = q^2 + q^3 + \\ldots + q^{T+1}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "s - qs &= q - q^{T+1} \\\\\n",
    "\\Rightarrow\\, s(1 - q) &= q(1 - q^T) \\\\\n",
    "\\Rightarrow\\, s &= \\frac{q(1 - q^T)}{1 - q}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we can write the empirical probability $p^*$ as\n",
    "\n",
    "$$\n",
    "p^* = 1 - \\frac{q(1 - q^T)}{T(1 - q)} = 1 - \\frac{(1-p)(1 - (1-p)^T)}{Tp}\n",
    "$$\n",
    "\n",
    "This cannot be solved analytically for $p$ in the case of arbitrary $T$, but it is easy to find numerical solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_star = 0.4\n",
    "max_prior_length = 15\n",
    "\n",
    "Ts = np.linspace(1, max_prior_length, max_prior_length, dtype=int)\n",
    "f = lambda p,t,p_star : 1 - (1 - p)*(1 - (1-p)**t) / (t*p) - p_star\n",
    "\n",
    "sols = []\n",
    "for t in Ts:\n",
    "    # find roots of f numerically\n",
    "    sols.append(sp.optimize.root_scalar(f, args=(t,p_star), bracket=[0.001, 0.999]).root)\n",
    "    \n",
    "# plot the computed solutions\n",
    "fig, ax = plt.subplots(figsize=set_size())\n",
    "ax.plot(Ts, sols, 'o--');\n",
    "ax.set_xlim([0, max_prior_length+1]);\n",
    "ax.set_xticks(Ts[::2]);\n",
    "ax.set_xlabel(\"length of prior $T$\");\n",
    "ax.set_ylabel(r\"solution for $\\tilde{b}_1$\");\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/simple_rate_decay.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/simple_rate_decay.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if this overly simplified model applies to the full HMM as well, we trained our model for 15 different uniform time priors and plotted the learned mean parameters. We also compared the risk predictions using these different time priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling parameters\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "moves = [(emcee.moves.DEMove(), 0.8), (emcee.moves.DESnookerMove(), 0.2)]\n",
    "\n",
    "# starting point\n",
    "np.random.seed(SEED)\n",
    "theta0 = np.random.uniform(low=0., high=1., size=(nwalker,ndim))\n",
    "prior_dict = {}\n",
    "\n",
    "samples_multiT_HMM = np.zeros(shape=(max_prior_length, \n",
    "                                     nwalker*(nstep-burnin), \n",
    "                                     ndim))\n",
    "\n",
    "for i in range(max_prior_length):\n",
    "    if DRAW_SAMPLES:\n",
    "        if __name__ == \"__main__\":\n",
    "            with Pool() as pool:\n",
    "                # uniform time prior of varying length\n",
    "                prior_dict['early'] = np.concatenate([[0.], [1./(i+1)]*(i+1)])\n",
    "\n",
    "                sampler = emcee.EnsembleSampler(nwalker, ndim, systm.likelihood, \n",
    "                                                args=[['early'], prior_dict], \n",
    "                                                kwargs={\"mode\": \"HMM\"},\n",
    "                                                moves=moves, pool=pool)\n",
    "                sampler.run_mcmc(theta0, nstep, progress=True)\n",
    "\n",
    "                # store the i-th sampling round\n",
    "                samples_multiT_HMM[i] = sampler.get_chain(flat=True, \n",
    "                                                          discard=burnin)\n",
    "\n",
    "        # saving the sampled data to disk for later convenience\n",
    "        np.save(f\"./samples/multiT_HMM_{i}.npy\", samples_multiT_HMM[i])\n",
    "        \n",
    "    else:\n",
    "        # loading in case we don't want to draw all the samples again\n",
    "        samples_multiT_HMM[i] = np.load(f\"./samples/multiT_HMM_{i}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results vs the theoretical values. This is only applicable to the base probability rates $\\tilde{b}_1$ and $\\tilde{b}_2$, since all other LNL's probability rates are also influenced by other efferent spread pathways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean and variance of sampled parameters\n",
    "multiT_b = [np.mean(samples_multiT_HMM[:,:,i], axis=1) for i in range(4)]\n",
    "multiT_b_var = [np.var(samples_multiT_HMM[:,:,i], axis=1) for i in range(4)]\n",
    "multiT_t = [np.mean(samples_multiT_HMM[1:,:,i], axis=1) for i in range(4,7)]\n",
    "multiT_t_var = [np.var(samples_multiT_HMM[1:,:,i], axis=1) for i in range(4,7)]\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1,2, figsize=set_size(\"full\", ratio=2.8))\n",
    "\n",
    "# plot base prob sample means for differently lengthed time priors\n",
    "for i in range(4):\n",
    "    ax[0].plot(range(1,max_prior_length+1), multiT_b[i], \"o\", mfc=\"none\", ms=4, \n",
    "               label=f\"$\\\\tilde{{b}}_{{{i+1}}}$ sampled\");\n",
    "\n",
    "# prevalence of involvement in LNL I and II\n",
    "prevalence_I = np.sum(data[(\"path\", \"I\")].to_numpy()) / len(data)\n",
    "prevalence_II = np.sum(data[(\"path\", \"II\")].to_numpy()) / len(data)\n",
    "theory_roots = np.zeros(shape=(2,max_prior_length))\n",
    "\n",
    "# compute roots with P^* = prevalence\n",
    "for t in range(1,max_prior_length+1):\n",
    "    theory_roots[0,t-1] = sp.optimize.root_scalar(f, \n",
    "                                                  args=(t, prevalence_I), \n",
    "                                                  bracket=[0.001, 0.999]).root\n",
    "    theory_roots[1,t-1] = sp.optimize.root_scalar(f, \n",
    "                                                  args=(t, prevalence_II), \n",
    "                                                  bracket=[0.001, 0.999]).root\n",
    "\n",
    "# plot theoretical roots\n",
    "ax[0].plot(range(1,max_prior_length+1), theory_roots[0], \"-\", alpha=0.5, \n",
    "           label=r\"$\\tilde{b}_{1}$ theory\");\n",
    "ax[0].plot(range(1,max_prior_length+1), theory_roots[1], \"-\", alpha=0.5, \n",
    "           label=r\"$\\tilde{b}_{2}$ theory\");\n",
    "\n",
    "offset = 0.5\n",
    "ax[0].set_xlim([1 - offset, max_prior_length + offset]);\n",
    "ax[0].set_xticks(np.arange(1, max_prior_length+1, 2))\n",
    "ax[0].set_xlabel(\"length of prior $T$\");\n",
    "ax[0].set_ylabel(r\"Base Probability Rate $\\tilde{b}$\");\n",
    "ax[0].legend(ncol=1);\n",
    "\n",
    "\n",
    "# plot trans prob sample means\n",
    "# plot sample means for differently lengthed time priors\n",
    "for i in range(3):\n",
    "    ax[1].plot(range(2,max_prior_length+1), multiT_t[i], \"o\", mfc=\"none\", ms=4, \n",
    "               label=f\"$\\\\tilde{{t}}_{{{i+1}}}$ sampled\");\n",
    "    \n",
    "offset = 0.5\n",
    "ax[1].set_xlim([1 - offset, max_prior_length + offset]);\n",
    "ax[1].set_xticks(np.arange(1, max_prior_length+1, 2))\n",
    "ax[1].set_xlabel(\"length of prior $T$\");\n",
    "ax[1].set_ylabel(r\"Transition Probability Rate $\\tilde{t}$\");\n",
    "ax[1].legend(ncol=1);\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/rate_decay_theory_vs_sampled.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/rate_decay_theory_vs_sampled.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show that the probability rates of a more complex and realistic system qualitatively follow the same behaviour as the example with only one node. This serves as an argument why we can essentially choose the length of the prior as it suits us.\n",
    "\n",
    "Finally, let's check if the risk actually changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract representation of the lymphatic network\n",
    "graph = {('tumor', 'primary')  : ['I', 'II', 'III', 'IV'], \n",
    "         ('lnl', 'I')          : ['II'], \n",
    "         ('lnl', 'II')         : ['III'], \n",
    "         ('lnl', 'III')        : ['IV'], \n",
    "         ('lnl', 'IV')         : []}\n",
    "\n",
    "systm = lymph.System(graph=graph)\n",
    "\n",
    "# set specificity & sensitivity of diagnostic modality (here CT) manually\n",
    "ct_spsn = {\"CT\": [0.76, 0.81]}\n",
    "systm.modalities = ct_spsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we want to know, what do we know?\n",
    "inv = np.array([None, None, 1, None])  # we're interested in the risk of lvl 3 being involved\n",
    "# our observation is that lvl 2 is involved\n",
    "obs = {\"CT\": np.array([0, 1, 0, 0])}\n",
    "\n",
    "np.random.seed(SEED)\n",
    "ndim, nwalker, nstep, burnin = 7, 200, 2000, 1000\n",
    "thin = 200\n",
    "time_dists = {}\n",
    "hmm_risk = np.zeros(shape=(max_prior_length, (nstep-burnin)*nwalker//thin))\n",
    "\n",
    "for k in range(max_prior_length):\n",
    "    # time priors\n",
    "    time_dists['early'] = np.concatenate([[0.], [1./(k+1)] * (k+1)])\n",
    "\n",
    "    # risk for HMM and two different \"T-stages\" (early and late)\n",
    "    subset = np.random.permutation(samples_multiT_HMM[k])[::thin]\n",
    "    for i, sample in enumerate(subset):\n",
    "        systm.spread_probs = sample\n",
    "        hmm_risk[k, i] = systm.risk(\n",
    "            inv=inv, diagnoses=obs, \n",
    "            time_dist=time_dists[\"early\"], mode=\"HMM\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_risk = np.zeros(shape=(len(samples_BN) // thin))\n",
    "for i, sample in enumerate(np.random.permutation(samples_BN)[::thin]):\n",
    "    systm.spread_probs = sample\n",
    "    bn_risk[i] = systm.risk(inv=inv, diagnoses=obs, mode=\"BN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=set_size())\n",
    "\n",
    "ax.hist(100*hmm_risk[0], bins=100, range=[0., 50], density=True, \n",
    "        color=usz_red, alpha=0.1, label=f\"HMM ({max_prior_length} different $T$)\")\n",
    "[ax.hist(100*hmm_risk[k], bins=100, range=[0., 50], density=True, \n",
    "         color=usz_red, alpha=0.1) for k in np.arange(1,max_prior_length)];\n",
    "ax.hist(100*bn_risk, histtype=\"step\", bins=100, linewidth=2, range=[0., 50], \n",
    "        density=True, label=\"BN\");\n",
    "ax.set_xlim([0., 20]);\n",
    "ax.legend();\n",
    "ax.set_xlabel(r\"risk $R$ [%]\");\n",
    "ax.set_ylabel(\"PDF of risk\");\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(\"./figures/multi_length_risk.png\", dpi=300, \n",
    "                bbox_inches=\"tight\")\n",
    "    plt.savefig(\"./figures/multi_length_risk.svg\", \n",
    "                bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The risk for all the HMMs is the same, except for the one that only includes one time step. The reason for this is that when the system is not given the time to spread, it cannot correctly estimate the conditional risks of what would happen if the previous node was involved vs if it was not involved. So the HMM that is \"too short\" overestimates the risk due to the high prevalence of LNL III's involvement."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "178d24c204a30672ea1fdde86877e76d36aba87ee79c03bdea66a58d070c8fdb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "235px",
    "width": "277px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
